{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[API KEY]\n",
      "sk-proj-2ALiWBzcJl4s9ri6EUJ6T3BlbkFJxbUDIanlzbIf6JMPE7o2\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(f\"[API KEY]\\n{os.environ['OPENAI_API_KEY']}\")\n",
    "print(os.environ[\"LANGCHAIN_TRACING_V2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/choi/anaconda3/envs/RAG_langchain_v02/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts.chat import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import transformers\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from transformers import pipeline, AutoModel, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.callbacks.manager import CallbackManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 페이지수 : 1255\n"
     ]
    }
   ],
   "source": [
    "# 단계 1 : 데이터셋 로드\n",
    "\n",
    "import json\n",
    "# JSON 파일에서 데이터를 불러와 Document 객체로 복원\n",
    "with open(\"/media/choi/HDD1/mmaction2/data/Korea_construction_standard/LHCS_qna_id.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    documents_data = json.load(f)\n",
    "\n",
    "documents_512 = [\n",
    "    Document(page_content=doc[\"page_content\"], metadata=doc[\"metadata\"]) for doc in documents_data\n",
    "]\n",
    "\n",
    "print(f\"문서의 페이지수 : {len(documents_512)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84613/1499260757.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  EM_klue_nli = HuggingFaceEmbeddings(model_name = '/home/choi/Git/ConSRoBERTa/output/2024_2/klue_nli_top_2e_55.24/kor_multi_klue-2024-09-19_15-26-29e1e2')\n",
      "No sentence-transformers model found with name klue/roberta-base. Creating a new one with mean pooling.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 단계 2 : 임베딩 모델 로드\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "# from FlagEmbedding import FlagModel\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "EM_klue_nli = HuggingFaceEmbeddings(model_name = '/home/choi/Git/ConSRoBERTa/output/2024_2/klue_nli_top_2e_55.24/kor_multi_klue-2024-09-19_15-26-29e1e2')\n",
    "EM_open_3 = OpenAIEmbeddings(model = 'text-embedding-3-large') # 최신 GPT4 유로 모델\n",
    "EM_klue_vanilla = HuggingFaceEmbeddings(model_name = 'klue/roberta-base')\n",
    "EM_MNRL_MRL = HuggingFaceEmbeddings(model_name='/home/choi/Git/RAG_con_doc/langchain/FT_model/MRL_MNRL_NLI-2024-10-17_14-16-30/checkpoint/checkpoint-322_best')\n",
    "bge_m3 = HuggingFaceEmbeddings(model_name = 'BAAI/bge-m3')\n",
    "e5_large = HuggingFaceEmbeddings(model_name = 'intfloat/multilingual-e5-large-instruct')\n",
    "\n",
    "\n",
    "\n",
    "embedding_models = [\n",
    "    EM_open_3,\n",
    "    bge_m3,\n",
    "    e5_large\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5검색 결과와 답변 생성까지 저장\n",
    "llm = ChatOpenAI(model_name='gpt-4o', temperature=0.0)\n",
    "\n",
    "answer_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "- <Question> 이후에 오는 질문은 <Text> 이후에 오는 문서와 관련된 질문이야.\n",
    "- <Text> 이후의 문서를 참고해서 <Question> 이후에 오는 질문에 대한 답변을 생성해줘.\n",
    "- 정확하고 간결하게 답변해줘\n",
    "- 답변은 한글로 출력해줘\n",
    "\n",
    "\n",
    "<Question> : {question}\n",
    "<Text>: {input}\n",
    "----\n",
    "<Answer>:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "class AnswerParser(StrOutputParser):\n",
    "    def parse(self, response):\n",
    "        # \"Question:\" 이후의 텍스트를 추출\n",
    "        if \"Answer>:\" in response:\n",
    "            return response.split(\"Answer>:\")[1].strip()\n",
    "        return response.strip()\n",
    "    \n",
    "A_chain = answer_prompt | llm | AnswerParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 테스트 함수 정의\n",
    "def Accuracy_test(documents, embed_model):\n",
    "    vectorstore = FAISS.from_documents(documents=documents, embedding=embed_model)\n",
    "    print(\"FAISS 인덱스 생성 완료\")\n",
    "\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    correct = 0\n",
    "    total = len(documents_512)\n",
    "\n",
    "    for doc in documents_512:\n",
    "        question = doc.metadata['question']\n",
    "        doc_id = doc.metadata['id']\n",
    "        \n",
    "        # FAISS를 사용해 질문과 가장 유사한 문서 검색\n",
    "        results = retriever.invoke(question)\n",
    "        retrieved_doc_id = results[0].metadata['id']  # 검색된 문서\n",
    "        \n",
    "        # 질문에 해당하는 답변과 검색된 문서 비교\n",
    "        if doc_id == retrieved_doc_id:\n",
    "            correct += 1\n",
    "            \n",
    "    # 정확도 계산\n",
    "    accuracy = (correct / total) * 100\n",
    "    print(f\"검색 정확도: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDCG와 MRR 점수 비교하고 결과 top5 랑 답변 결과까지 저장하기\n",
    "import json\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# FAISS로 검색 후 상위 5개의 검색 결과를 저장하고, NDCG@5, MRR@5를 계산하는 함수\n",
    "def evaluate_and_save_results(embedding_model, documents, output_file, A_chain):\n",
    "    \n",
    "    vectorstore = FAISS.from_documents(documents=documents, embedding=embedding_model)\n",
    "    \n",
    "    results = []\n",
    "    ndcg_scores = []\n",
    "    mrr_scores = []\n",
    "\n",
    "    for doc in documents:\n",
    "        question = doc.metadata['question']\n",
    "        correct_id = doc.metadata['id']\n",
    "        correct_answer = doc.metadata['answer']\n",
    "\n",
    "        # FAISS로 상위 5개 검색\n",
    "        retrieved_results = vectorstore.similarity_search(query=question,k=5)\n",
    "\n",
    "        # 검색된 문서들의 ID를 저장\n",
    "        retrieved_ids = [result.metadata['id'] for result in retrieved_results]\n",
    "\n",
    "        # 상위 1개의 검색 결과로 답변 생성\n",
    "        top_document = retrieved_results[0].page_content\n",
    "        generated_answer = A_chain.invoke({\n",
    "            'question': question,\n",
    "            'input': top_document\n",
    "        })\n",
    "\n",
    "        # 정답 ID를 NDCG와 MRR 계산용으로 변환\n",
    "        y_true = [1 if retrieved_id == correct_id else 0 for retrieved_id in retrieved_ids]\n",
    "        y_score = [5, 4, 3, 2, 1]  # 순위에 따른 가중치\n",
    "\n",
    "        # NDCG@5 계산\n",
    "        ndcg = ndcg_score([y_true], [y_score], k=5)\n",
    "        ndcg_scores.append(ndcg)\n",
    "\n",
    "        # MRR@5 계산\n",
    "        try:\n",
    "            rank = retrieved_ids.index(correct_id) + 1\n",
    "            mrr = 1 / rank\n",
    "        except ValueError:\n",
    "            mrr = 0  # 정답이 top 5 안에 없으면 MRR은 0\n",
    "        mrr_scores.append(mrr)\n",
    "\n",
    "        # 결과를 저장할 데이터 구성\n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'correct_id': correct_id,\n",
    "            'retrieved_ids': retrieved_ids,\n",
    "            'correct_answer': correct_answer,\n",
    "            'generated_answer': generated_answer,  # 쉼표 누락 수정\n",
    "            'ndcg@5': ndcg,\n",
    "            'mrr@5': mrr\n",
    "        })\n",
    "\n",
    "    # 결과를 JSON 파일로 저장\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # NDCG@5, MRR@5 평균 출력\n",
    "    print(f\"Average NDCG@5: {sum(ndcg_scores) / len(ndcg_scores)}\")\n",
    "    print(f\"Average MRR@5: {sum(mrr_scores) / len(mrr_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: EM_klue_nli\n",
      "Average NDCG@5: 0.6903726108736176\n",
      "Average MRR@5: 0.6620717131474109\n"
     ]
    }
   ],
   "source": [
    "# 간단한 테스트\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# 임베딩 모델 로드 (파인튜닝된 모델)\n",
    "\n",
    "# Accuracy_test(documents_512, EM_klue_nli)\n",
    "# Accuracy_test(documents_512, EM_open)\n",
    "# Accuracy_test(documents_512, EM_klue_vanilla)\n",
    "# output_path = '/home/choi/Git/RAG_con_doc/langchain/MNRL_MRL.json'\n",
    "# evaluate_and_save_results(embed_model, documents_512_sample, output_path, A_chain)\n",
    "embedding_models2 = [\n",
    "    ('EM_klue_nli', EM_klue_nli),\n",
    "    # ('EM_MNRL_MRL', EM_MNRL_MRL),\n",
    "    # ('EM_open', EM_open),\n",
    "    # ('EM_klue_vanilla', EM_klue_vanilla),\n",
    "    # ('EM_open_3', EM_open_3),\n",
    "    # ('bge_m3', bge_m3),\n",
    "    # ('e5_large', e5_large),\n",
    "    \n",
    "]\n",
    "\n",
    "for model_name, embed_model in embedding_models2:\n",
    "    output_path = f'/home/choi/Git/RAG_con_doc/langchain/{model_name}_results.json'\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "    evaluate_and_save_results(embed_model, documents_512, output_path, A_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 아래는 간단한 테스트를 위한 코드이므로 삭제해도 괜찮음\n",
    "def calculate_accuracy(file_path):\n",
    "    # JSON 파일 불러오기\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Initialize counters\n",
    "    correct_count = 0\n",
    "    total_count = len(data)\n",
    "\n",
    "    # Iterate through each entry and check if the correct_id matches the first retrieved_id\n",
    "    for entry in data:\n",
    "        if entry[\"correct_id\"] == entry[\"retrieved_ids\"][0]:\n",
    "            correct_count += 1\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_count / total_count * 100\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "def calculate_accuracy_and_average_scores(file_path):\n",
    "    # JSON 파일 불러오기\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Initialize counters\n",
    "    correct_count = 0\n",
    "    total_count = len(data)\n",
    "    ndcg_sum = 0\n",
    "    mrr_sum = 0\n",
    "\n",
    "    # Iterate through each entry to calculate correct count, ndcg sum, and mrr sum\n",
    "    for entry in data:\n",
    "        # Check if correct_id matches the first retrieved_id for accuracy calculation\n",
    "        if entry[\"correct_id\"] == entry[\"retrieved_ids\"][0]:\n",
    "            correct_count += 1\n",
    "\n",
    "        # Sum up ndcg@5 and mrr@5 scores\n",
    "        ndcg_sum += entry.get(\"ndcg@5\", 0)\n",
    "        mrr_sum += entry.get(\"mrr@5\", 0)\n",
    "\n",
    "    # Calculate accuracy, ndcg@5 average, and mrr@5 average\n",
    "    accuracy = (correct_count / total_count) * 100\n",
    "    avg_ndcg = ndcg_sum / total_count\n",
    "    avg_mrr = mrr_sum / total_count\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Average NDCG@5: {avg_ndcg:.4f}\")\n",
    "    print(f\"Average MRR@5: {avg_mrr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: EM_klue_nli\n",
      "Accuracy: 58.65%\n",
      "Average NDCG@5: 0.6904\n",
      "Average MRR@5: 0.6621\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for model_name, embed_model in embedding_models2:\n",
    "    file_path = f'/home/choi/Git/RAG_con_doc/langchain/{model_name}_results.json'\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "    calculate_accuracy_and_average_scores(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.60%\n",
      "Average NDCG@5: 0.7454\n",
      "Average MRR@5: 0.7047\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy_and_average_scores('/home/choi/Git/RAG_con_doc/langchain/EM_MNRL_MRL_results.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_langchain_v02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
