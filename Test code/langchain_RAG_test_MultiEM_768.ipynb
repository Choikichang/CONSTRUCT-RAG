{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[API KEY]\n",
      "sk-proj-2ALiWBzcJl4s9ri6EUJ6T3BlbkFJxbUDIanlzbIf6JMPE7o2\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(f\"[API KEY]\\n{os.environ['OPENAI_API_KEY']}\")\n",
    "print(os.environ[\"LANGCHAIN_TRACING_V2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts.chat import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import transformers\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from transformers import pipeline, AutoModel, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.callbacks.manager import CallbackManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 페이지수 : 1255\n"
     ]
    }
   ],
   "source": [
    "# 단계 1 : 데이터셋 로드\n",
    "\n",
    "import json\n",
    "# JSON 파일에서 데이터를 불러와 Document 객체로 복원\n",
    "with open(\"/media/choi/HDD1/mmaction2/data/Korea_construction_standard/LHCS_qna_id.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    documents_data = json.load(f)\n",
    "\n",
    "documents_512 = [\n",
    "    Document(page_content=doc[\"page_content\"], metadata=doc[\"metadata\"]) for doc in documents_data\n",
    "]\n",
    "\n",
    "print(f\"문서의 페이지수 : {len(documents_512)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단계 2 : 임베딩 모델 로드\n",
    "# EM_klue_nli = HuggingFaceEmbeddings(model_name = '/home/choi/Git/ConSRoBERTa/output/2024_2/klue_nli_top_2e_55.24/kor_multi_klue-2024-09-19_15-26-29e1e2')\n",
    "# EM_open = OpenAIEmbeddings(model = 'text-embedding-3-large') # 최신 GPT4 유로 모델\n",
    "# EM_klue_vanilla = HuggingFaceEmbeddings(model_name = 'klue/roberta-base')\n",
    "EM_MNRL_MRL = HuggingFaceEmbeddings(model_name='/home/choi/Git/RAG_con_doc/langchain/FT_model/MRL_MNRL_NLI-2024-10-17_14-16-30/checkpoint/checkpoint-322_best')\n",
    "# embedding_models = [\n",
    "#     EM_klue_nli,\n",
    "#     EM_MNRL_MRL,\n",
    "#     EM_open,\n",
    "#     EM_klue_vanilla,\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5검색 결과와 답변 생성까지 저장\n",
    "llm = ChatOpenAI(model_name='gpt-4o', temperature=0.0)\n",
    "\n",
    "answer_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "- <Question> 이후에 오는 질문은 <Text> 이후에 오는 문서와 관련된 질문이야.\n",
    "- <Text> 이후의 문서를 참고해서 <Question> 이후에 오는 질문에 대한 답변을 생성해줘.\n",
    "- 정확하고 간결하게 답변해줘\n",
    "- 답변은 한글로 출력해줘\n",
    "\n",
    "\n",
    "<Question> : {question}\n",
    "<Text>: {input}\n",
    "----\n",
    "<Answer>:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "class AnswerParser(StrOutputParser):\n",
    "    def parse(self, response):\n",
    "        # \"Question:\" 이후의 텍스트를 추출\n",
    "        if \"Answer>:\" in response:\n",
    "            return response.split(\"Answer>:\")[1].strip()\n",
    "        return response.strip()\n",
    "    \n",
    "A_chain = answer_prompt | llm | AnswerParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 단계 : 768d 를 자르고 임베딩한 후 여러 방법으로 합쳐서 하나의 768d 벡터 만들기\n",
    "def Accuracy_test(documents, embed_model, model_name, chunk_size):\n",
    "    \n",
    "    # 1. Langchain의 RecursiveCharacterTextSplitter 사용\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "\n",
    "    # 모델 불러오기 (HuggingFaceEmbeddings 사용)\n",
    "    # embed_model = HuggingFaceEmbeddings(model_name='/home/choi/Git/RAG_con_doc/langchain/FT_model/MRL_MNRL_NLI-2024-10-17_14-16-30/checkpoint/checkpoint-322_best')\n",
    "\n",
    "    # 문서 예시 (documents_512에서 첫 번째 문서)\n",
    "    for doc in documents:\n",
    "        \n",
    "        text = doc.page_content\n",
    "        metadata = doc.metadata\n",
    "        # 2. 임베딩 벡터와 관련된 기존 데이터 초기화\n",
    "        for key in list(metadata.keys()):\n",
    "            if 'embedding_vector' in key or 'num_chunks' in key:\n",
    "                del metadata[key]  # 이전 임베딩 관련 데이터 삭제\n",
    "                \n",
    "        # 2. RecursiveCharacterTextSplitter로 문서를 128토큰 단위로 분할\n",
    "        split_documents = splitter.split_text(text)\n",
    "\n",
    "        # 3. 각 분할된 텍스트를 임베딩\n",
    "        chunk_embeddings = [embed_model.embed_query(chunk) for chunk in split_documents]\n",
    "\n",
    "##################################combined_embedding 한개만 쓰면됨################################################################\n",
    "\n",
    "        # 4. 768 차원으로 결합하는 방식 (method에 따라 다름)\n",
    "        if model_name == \"768_Mean\":\n",
    "            combined_embedding = np.mean(chunk_embeddings, axis=0)\n",
    "        elif model_name == \"768_First\":\n",
    "            combined_embedding = np.concatenate([chunk[:768 // len(chunk_embeddings)] for chunk in chunk_embeddings], axis=0)\n",
    "        elif model_name == \"768_Each\":\n",
    "            num_chunks = len(chunk_embeddings)\n",
    "            split_size = 768 // num_chunks  # 각 청크에서 가져올 크기 계산\n",
    "            combined_embedding = []\n",
    "\n",
    "            # 각 청크에서 순차적으로 부분을 가져오는 방식\n",
    "            for i, chunk in enumerate(chunk_embeddings):\n",
    "                start_index = i * split_size\n",
    "                end_index = (i + 1) * split_size\n",
    "\n",
    "                # 각 청크에서 해당 부분을 결합\n",
    "                combined_embedding.append(chunk[start_index:end_index])\n",
    "\n",
    "            combined_embedding = np.concatenate(combined_embedding, axis=0)\n",
    "\n",
    "            # 결합된 벡터가 768차원이 안 되면 마지막 청크에서 부족한 부분을 가져와 채우기\n",
    "            if combined_embedding.shape[0] < 768:\n",
    "                remaining_size = 768 - combined_embedding.shape[0]\n",
    "                combined_embedding = np.concatenate([combined_embedding, chunk_embeddings[-1][:remaining_size]], axis=0)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method specified.\")\n",
    "\n",
    "        # Metadata에 벡터 저장\n",
    "        metadata[f'{model_name}embedding_vector'] = combined_embedding.tolist()\n",
    "        metadata['num_chunks'] = len(chunk_embeddings)\n",
    "\n",
    "        # print(f\"Document ID: {metadata['id']} - Embedding created with {model_name}\")\n",
    "\n",
    "\n",
    "##################################combined_embedding 한개만 쓰면됨################################################################\n",
    "\n",
    "    # 문서를 딕셔너리로 변환하여 JSON으로 저장\n",
    "    documents = [{\"page_content\": doc.page_content, \"metadata\": doc.metadata} for doc in documents]\n",
    "\n",
    "    with open(f'/home/choi/Git/RAG_con_doc/langchain/{model_name}_{chunk_size}_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(documents, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 단계 : 768d 를 자르고 임베딩한 후 concatenation 하기\n",
    "def Accuracy_test(documents, embed_model, model_name, chunk_size):\n",
    "    \n",
    "    # 1. Langchain의 RecursiveCharacterTextSplitter 사용\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "\n",
    "    # 모델 불러오기 (HuggingFaceEmbeddings 사용)\n",
    "    # embed_model = HuggingFaceEmbeddings(model_name='/home/choi/Git/RAG_con_doc/langchain/FT_model/MRL_MNRL_NLI-2024-10-17_14-16-30/checkpoint/checkpoint-322_best')\n",
    "\n",
    "    # 문서 예시 (documents_512에서 첫 번째 문서)\n",
    "    for doc in documents:\n",
    "        \n",
    "        text = doc.page_content\n",
    "        metadata = doc.metadata\n",
    "        # 2. 임베딩 벡터와 관련된 기존 데이터 초기화\n",
    "        for key in list(metadata.keys()):\n",
    "            if 'embedding_vector' in key or 'num_chunks' in key:\n",
    "                del metadata[key]  # 이전 임베딩 관련 데이터 삭제\n",
    "                \n",
    "        # 2. RecursiveCharacterTextSplitter로 문서를 128토큰 단위로 분할\n",
    "        split_documents = splitter.split_text(text)\n",
    "\n",
    "        # 3. 각 분할된 텍스트를 임베딩\n",
    "        chunk_embeddings = [embed_model.embed_query(chunk) for chunk in split_documents]\n",
    "\n",
    "##################################combined_embedding 한개만 쓰면됨################################################################\n",
    "\n",
    "        # 4. 768 차원으로 결합하는 방식 (method에 따라 다름)\n",
    "        if model_name == \"768_Mean\":\n",
    "            combined_embedding = np.mean(chunk_embeddings, axis=0)\n",
    "        elif model_name == \"768_First\":\n",
    "            combined_embedding = np.concatenate([chunk[:768 // len(chunk_embeddings)] for chunk in chunk_embeddings], axis=0)\n",
    "        elif model_name == \"768_Each\":\n",
    "            num_chunks = len(chunk_embeddings)\n",
    "            split_size = 768 // num_chunks  # 각 청크에서 가져올 크기 계산\n",
    "            combined_embedding = []\n",
    "\n",
    "            # 각 청크에서 순차적으로 부분을 가져오는 방식\n",
    "            for i, chunk in enumerate(chunk_embeddings):\n",
    "                start_index = i * split_size\n",
    "                end_index = (i + 1) * split_size\n",
    "\n",
    "                # 각 청크에서 해당 부분을 결합\n",
    "                combined_embedding.append(chunk[start_index:end_index])\n",
    "\n",
    "            combined_embedding = np.concatenate(combined_embedding, axis=0)\n",
    "\n",
    "            # 결합된 벡터가 768차원이 안 되면 마지막 청크에서 부족한 부분을 가져와 채우기\n",
    "            if combined_embedding.shape[0] < 768:\n",
    "                remaining_size = 768 - combined_embedding.shape[0]\n",
    "                combined_embedding = np.concatenate([combined_embedding, chunk_embeddings[-1][:remaining_size]], axis=0)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method specified.\")\n",
    "\n",
    "        # Metadata에 벡터 저장\n",
    "        metadata[f'{model_name}embedding_vector'] = combined_embedding.tolist()\n",
    "        metadata['num_chunks'] = len(chunk_embeddings)\n",
    "\n",
    "        # print(f\"Document ID: {metadata['id']} - Embedding created with {model_name}\")\n",
    "\n",
    "\n",
    "##################################combined_embedding 한개만 쓰면됨################################################################\n",
    "\n",
    "    # 문서를 딕셔너리로 변환하여 JSON으로 저장\n",
    "    documents = [{\"page_content\": doc.page_content, \"metadata\": doc.metadata} for doc in documents]\n",
    "\n",
    "    with open(f'/home/choi/Git/RAG_con_doc/langchain/{model_name}_{chunk_size}_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(documents, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 페이지 수: 1255\n",
      "정확도: 3.82%\n"
     ]
    }
   ],
   "source": [
    "# 검색 함수 구현 (L2 거리 기반)\n",
    "def search_documents(query_embedding, documents):\n",
    "    # 검색할 모든 문서의 벡터와 L2 거리 계산\n",
    "    distances = []\n",
    "    for doc in documents:\n",
    "        doc_num_chunks = doc.metadata['num_chunks']\n",
    "        doc_vector = np.array(doc.metadata[f'{model_name}embedding_vector'])  # metadata에 저장된 벡터 사용\n",
    "        distance = np.linalg.norm(query_embedding - doc_vector)  # L2 거리 계산\n",
    "        std_dev = np.std(query_embedding - doc_vector)\n",
    "        distances.append((doc, distance * doc_num_chunks**0.5))\n",
    "    \n",
    "    # 거리 기준으로 가장 가까운 문서 5개 찾기\n",
    "    distances.sort(key=lambda x: x[1])  # 거리가 작은 순서대로 정렬\n",
    "    top_5 = distances[:5]  # 상위 5개의 문서\n",
    "    \n",
    "    # 상위 5개 문서 ID를 추출해서 metadata에 추가\n",
    "    top_5_ids = [doc.metadata['id'] for doc, _ in top_5]\n",
    "    \n",
    "    return top_5, top_5_ids\n",
    "\n",
    "# 정확도 계산 함수\n",
    "def calculate_accuracy(documents, embed_model):\n",
    "    correct_count = 0\n",
    "\n",
    "    for doc in documents:\n",
    "        question = doc.metadata['question']  # 질문을 metadata에서 불러옴\n",
    "        correct_doc_id = doc.metadata['id']  # 정답 문서 ID\n",
    "\n",
    "        # 질문 임베딩\n",
    "        query_embedding = embed_model.embed_query(question)\n",
    "\n",
    "        # 검색\n",
    "        top_5, top_5_ids = search_documents(query_embedding, documents)\n",
    "        \n",
    "        # 상위 5개 문서 중 가장 가까운 문서의 ID와 정답 ID 비교\n",
    "        if top_5_ids[0] == correct_doc_id:\n",
    "            correct_count += 1\n",
    "\n",
    "        # 검색된 문서에 상위 5개의 ID를 metadata에 저장\n",
    "        doc.metadata['top_5_ids'] = top_5_ids\n",
    "\n",
    "    # 정확도 계산 (정답 문서와 일치한 비율)\n",
    "    accuracy = correct_count / len(documents)\n",
    "    print(f\"정확도: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # 업데이트된 문서를 저장\n",
    "    documents_dict = [{\"page_content\": doc.page_content, \"metadata\": doc.metadata} for doc in documents]\n",
    "    with open('/home/choi/Git/RAG_con_doc/langchain/updated_768_Each_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(documents_dict, f, ensure_ascii=False, indent=4)\n",
    "# JSON 파일에서 데이터를 불러와 Document 객체로 변환\n",
    "model_name = '768_Each'\n",
    "\n",
    "with open(f\"/home/choi/Git/RAG_con_doc/langchain/{model_name}_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=doc[\"page_content\"], metadata=doc[\"metadata\"]) for doc in documents\n",
    "]\n",
    "\n",
    "print(f\"문서의 페이지 수: {len(documents)}\")\n",
    "# 예시 실행\n",
    "calculate_accuracy(documents, embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbeddings(model_name='/home/choi/Git/RAG_con_doc/langchain/FT_model/MRL_MNRL_NLI-2024-10-17_14-16-30/checkpoint/checkpoint-322_best')\n",
    "embedding_models2 = [\n",
    "    # ('EM_klue_nli', EM_klue_nli),\n",
    "    # (embed_model, '768_Mean'),\n",
    "    # (embed_model, '768_First'),\n",
    "    (embed_model, '768_Each'),\n",
    "]\n",
    "for embed_model, model_name in embedding_models2:\n",
    "    Accuracy_test(documents_512, embed_model, model_name, 256)\n",
    "# Accuracy_test(documents_512, embed_model, \"768_Mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDCG와 MRR 점수 비교하고 결과 top5 랑 답변 결과까지 저장하기\n",
    "import json\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# FAISS로 검색 후 상위 5개의 검색 결과를 저장하고, NDCG@5, MRR@5를 계산하는 함수\n",
    "def evaluate_and_save_results(embedding_model, documents, output_file, A_chain):\n",
    "    \n",
    "    vectorstore = FAISS.from_documents(documents=documents, embedding=embedding_model)\n",
    "    \n",
    "    results = []\n",
    "    ndcg_scores = []\n",
    "    mrr_scores = []\n",
    "\n",
    "    for doc in documents:\n",
    "        question = doc.metadata['question']\n",
    "        correct_id = doc.metadata['id']\n",
    "        correct_answer = doc.metadata['answer']\n",
    "\n",
    "        # FAISS로 상위 5개 검색\n",
    "        retrieved_results = vectorstore.similarity_search(query=question,k=5)\n",
    "\n",
    "        # 검색된 문서들의 ID를 저장\n",
    "        retrieved_ids = [result.metadata['id'] for result in retrieved_results]\n",
    "\n",
    "        # 상위 1개의 검색 결과로 답변 생성\n",
    "        top_document = retrieved_results[0].page_content\n",
    "        generated_answer = A_chain.invoke({\n",
    "            'question': question,\n",
    "            'input': top_document\n",
    "        })\n",
    "\n",
    "        # 정답 ID를 NDCG와 MRR 계산용으로 변환\n",
    "        y_true = [1 if retrieved_id == correct_id else 0 for retrieved_id in retrieved_ids]\n",
    "        y_score = [5, 4, 3, 2, 1]  # 순위에 따른 가중치\n",
    "\n",
    "        # NDCG@5 계산\n",
    "        ndcg = ndcg_score([y_true], [y_score], k=5)\n",
    "        ndcg_scores.append(ndcg)\n",
    "\n",
    "        # MRR@5 계산\n",
    "        try:\n",
    "            rank = retrieved_ids.index(correct_id) + 1\n",
    "            mrr = 1 / rank\n",
    "        except ValueError:\n",
    "            mrr = 0  # 정답이 top 5 안에 없으면 MRR은 0\n",
    "        mrr_scores.append(mrr)\n",
    "\n",
    "        # 결과를 저장할 데이터 구성\n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'correct_id': correct_id,\n",
    "            'retrieved_ids': retrieved_ids,\n",
    "            'correct_answer': correct_answer,\n",
    "            'generated_answer': generated_answer,  # 쉼표 누락 수정\n",
    "            'ndcg@5': ndcg,\n",
    "            'mrr@5': mrr\n",
    "        })\n",
    "\n",
    "    # 결과를 JSON 파일로 저장\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # NDCG@5, MRR@5 평균 출력\n",
    "    print(f\"Average NDCG@5: {sum(ndcg_scores) / len(ndcg_scores)}\")\n",
    "    print(f\"Average MRR@5: {sum(mrr_scores) / len(mrr_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: EM_MNRL_MRL\n",
      "Average NDCG@5: 0.7454216677828364\n",
      "Average MRR@5: 0.7047144754316079\n",
      "Evaluating model: EM_open\n"
     ]
    }
   ],
   "source": [
    "# 간단한 테스트\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# 임베딩 모델 로드 (파인튜닝된 모델)\n",
    "\n",
    "# Accuracy_test(documents_512, EM_klue_nli)\n",
    "# Accuracy_test(documents_512, EM_open)\n",
    "# Accuracy_test(documents_512, EM_klue_vanilla)\n",
    "# output_path = '/home/choi/Git/RAG_con_doc/langchain/MNRL_MRL.json'\n",
    "# evaluate_and_save_results(embed_model, documents_512_sample, output_path, A_chain)\n",
    "embedding_models2 = [\n",
    "    # ('EM_klue_nli', EM_klue_nli),\n",
    "    ('EM_MNRL_MRL', EM_MNRL_MRL),\n",
    "    ('EM_open', EM_open),\n",
    "    ('EM_klue_vanilla', EM_klue_vanilla),\n",
    "]\n",
    "\n",
    "for model_name, embed_model in embedding_models2:\n",
    "    output_path = f'/home/choi/Git/RAG_con_doc/langchain/{model_name}_results.json'\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "    evaluate_and_save_results(embed_model, documents_512, output_path, A_chain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
